{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Self-Supervised Learning â€“ Denoising Autoencoder (CIFAR-10)\n",
                "\n",
                "This notebook implements the task of comparing a standard CNN classifier to a Denoising Autoencoder (DAE) based approach.\n",
                "\n",
                "## Task Requirements:\n",
                "a) Train a CNN to discriminate between cats and dogs using CIFAR-10 data.\n",
                "b) Pre-train a denoising autoencoder on images from CIFAR-10 (except for cats and dogs).\n",
                "c) Fine-tune the pre-trained model to discriminate between cats and dogs.\n",
                "d) Experiment with the amount of fine-tuning data and the number of training steps. Compare convergence and performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "\n",
                "import os\n",
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "from src.experiments import run_baseline, run_pretraining, run_finetuning, plot_results\n",
                "\n",
                "# Ensure results directory exists\n",
                "os.makedirs('results', exist_ok=True)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (a): Baseline CNN\n",
                "Train a standard CNN to discriminate between cats and dogs using CIFAR-10 data.\n",
                "We will run this baseline with different data fractions (10%, 25%, 50%, 100%) to fully compare with the DAE approach."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run Baseline (using varying data fractions)\n",
                "# You can adjust epochs if needed for faster checking\n",
                "baseline_results = run_baseline(epochs=10, fractions=[0.1, 0.25, 0.5, 1.0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (b): DAE Pre-training\n",
                "Pre-train a denoising autoencoder on images from CIFAR-10 (except for cats and dogs)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pre-train DAE on non-cat/dog classes\n",
                "# This saves the encoder weights to 'results/pretrained_encoder.pth'\n",
                "dae_model, dae_history = run_pretraining(epochs=20)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part (c) & (d): Fine-tuning & Experiments\n",
                "Fine-tune the pre-trained model to discriminate between cats and dogs.\n",
                "**Ablation Study**: We use a lower learning rate (1e-4) to prevent catastrophic forgetting of pre-trained features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pretrained_path = os.path.join('results', 'pretrained_encoder.pth')\n",
                "\n",
                "# Run fine-tuning experiments with different data fractions\n",
                "# This returns a dictionary of results for each fraction\n",
                "finetune_results = run_finetuning(\n",
                "    pretrained_encoder_path=pretrained_path, \n",
                "    fractions=[0.1, 0.25, 0.5, 1.0],\n",
                "    epochs=10,\n",
                "    lr=0.0001\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analysis & Plotting\n",
                "Compare training time, convergence, and final performance. Does the pre-trained model need less data?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_results()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}